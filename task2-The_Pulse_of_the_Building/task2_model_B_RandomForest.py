import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import os
import joblib
import warnings

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# ================= é…ç½®åŒºåŸŸ (Configuration) =================
# è¾“å…¥ï¼šæ–¹æ¡ˆ A (K-Means) ç”Ÿæˆçš„å¸¦æ ‡ç­¾æ•°æ®
# ç¡®ä¿è¿™ä¸ªè·¯å¾„æŒ‡å‘ä½  Task 2 æ–¹æ¡ˆ A ç”Ÿæˆçš„æœ€æ–°ç»“æœæ–‡ä»¶
INPUT_FILE = "./task2-The_Pulse_of_the_Building/output/model_A_kmeans/results/model_A_kmeans_clustered_results.csv"

# è¾“å‡ºè·¯å¾„
BASE_OUT_DIR = "./task2-The_Pulse_of_the_Building/output/model_B_RandomForest"
IMG_DIR = os.path.join(BASE_OUT_DIR, "images")
RES_DIR = os.path.join(BASE_OUT_DIR, "results")
FILE_PREFIX = "model_B_RandomForest"

# è¶…å‚æ•°æœç´¢èŒƒå›´ (Grid Search)
# æ³¨æ„ï¼šå¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥å‡å°‘ n_estimators çš„é€‰é¡¹æˆ–å°† n_jobs è®¾ä¸º 1
# PARAM_GRID = {
#     'n_estimators': [50, 100, 150, 200, 250, 300],      # æ ‘çš„æ•°é‡
#     'max_depth': [None, 10, 20, 30, 100],          # æ ‘çš„æœ€å¤§æ·±åº¦
#     'min_samples_split': [3, 4, 5, 6, 7]               # èŠ‚ç‚¹åˆ†è£‚æœ€å°æ ·æœ¬æ•°
# }
PARAM_GRID = {
    'n_estimators': [245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255],      # æ ‘çš„æ•°é‡
    'max_depth': [None],          # æ ‘çš„æœ€å¤§æ·±åº¦
    'min_samples_split': [5]               # èŠ‚ç‚¹åˆ†è£‚æœ€å°æ ·æœ¬æ•°
}


# ç»˜å›¾é£æ ¼
plt.style.use('bmh')
# ==========================================================

def run_model_B_rf_optimized():
    print(f"ğŸš€ å¯åŠ¨ä»»åŠ¡äºŒ [æ–¹æ¡ˆ B: Random Forest] è¶…å‚æ•°å¯»ä¼˜å¢å¼ºç‰ˆ...")
    print(f"   ğŸ“‚ ç›®æ ‡è¾“å‡ºç›®å½•: {BASE_OUT_DIR}")
    
    # 1. ç›®å½•å‡†å¤‡
    for d in [IMG_DIR, RES_DIR]:
        if not os.path.exists(d):
            os.makedirs(d)
            print(f"   âœ… å·²åˆ›å»ºç›®å½•: {d}")

    # 2. è¯»å–æ•°æ®
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {INPUT_FILE}")
        print("   è¯·å…ˆè¿è¡Œæ–¹æ¡ˆ A (task2_model_A_kmeans_v2.py) ç”Ÿæˆæ ‡ç­¾ã€‚")
        return
    
    df = pd.read_csv(INPUT_FILE)
    print(f"   å·²åŠ è½½æ•°æ®: {len(df)} è¡Œ")
    
    # 3. å‡†å¤‡ç‰¹å¾ä¸æ ‡ç­¾
    # K-Means èšç±»ä½¿ç”¨çš„æ ¸å¿ƒç‰¹å¾ï¼ŒåŠ ä¸Š 'Cluster_Label' ä½œä¸ºç›®æ ‡
    features = ['Total_Load_kg', 'Total_Calls', 'Up_Ratio', 'Down_Ratio', 'Hour']
    X = df[features]
    y = df['Cluster_Label']
    
    # è·å– "Label ID" åˆ° "Label Name" çš„æ˜ å°„å­—å…¸ (ç”¨äºç”»å›¾æ˜¾ç¤ºçœŸå®åå­—)
    label_map = df[['Cluster_Label', 'Cluster_Name']].drop_duplicates().set_index('Cluster_Label')['Cluster_Name'].to_dict()
    # æ’åºåçš„æ ‡ç­¾åç§°åˆ—è¡¨
    unique_labels = sorted(list(set(y)))
    label_names = [label_map[i] for i in unique_labels]
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (80% è®­ç»ƒ, 20% æµ‹è¯•)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ==========================================
    # 4. è¶…å‚æ•°ç½‘æ ¼æœç´¢ (Grid Search)
    # ==========================================
    print(f"ğŸ” å¼€å§‹è¶…å‚æ•°å¯»ä¼˜ (Grid Search)...")
    print(f"   å‚æ•°ç©ºé—´: {PARAM_GRID}")
    
    rf = RandomForestClassifier(random_state=42)
    
    # n_jobs=-1 ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒã€‚å¦‚æœå†…å­˜ä¸è¶³æŠ¥é”™ï¼Œè¯·æ”¹ä¸º n_jobs=2 æˆ– n_jobs=1
    grid_search = GridSearchCV(estimator=rf, param_grid=PARAM_GRID, cv=3, n_jobs=-1, verbose=1)
    grid_search.fit(X_train, y_train)
    
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    
    print(f"\nğŸ† æœ€ä½³å‚æ•°ç»„åˆ found:")
    print(f"   {best_params}")
    print(f"   è®­ç»ƒé›†éªŒè¯å¾—åˆ†: {best_score:.4f}")

    # ==========================================
    # 5. æ¨¡å‹è¯„ä¼°ä¸ç»“æœä¿å­˜ (å¢å¼ºç‰ˆ)
    # ==========================================
    y_pred = best_model.predict(X_test)
    test_acc = accuracy_score(y_test, y_pred)
    
    print(f"\nğŸ“Š === æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼° ===")
    print(f"   Test Accuracy: {test_acc:.4f}")

    # [æ–‡ä»¶1] åŸºç¡€æŒ‡æ ‡ (Metrics)
    metrics_path = os.path.join(RES_DIR, f"{FILE_PREFIX}_metrics.csv")
    pd.DataFrame([{
        'Model': 'Random Forest (Optimized)',
        'Accuracy': test_acc,
        'Best_Params': str(best_params),
        'Training_Set_Size': len(X_train),
        'Test_Set_Size': len(X_test)
    }]).to_csv(metrics_path, index=False)
    
    # [æ–‡ä»¶2] è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (Classification Report) - æ–°å¢ï¼
    # è¿™ä¼šä¿å­˜æ¯ä¸ªç±»åˆ«çš„ Precision, Recall, F1-Score
    report_dict = classification_report(y_test, y_pred, output_dict=True, target_names=label_names)
    report_df = pd.DataFrame(report_dict).transpose()
    report_path = os.path.join(RES_DIR, f"{FILE_PREFIX}_classification_report.csv")
    report_df.to_csv(report_path)
    print(f"   ğŸ’¾ [æ–°å¢] è¯¦ç»†åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # [æ–‡ä»¶3] ä¿å­˜æ¨¡å‹å¯¹è±¡ (.pkl) - æ–°å¢ï¼
    # æ–¹ä¾¿ Task 3 ç›´æ¥åŠ è½½ä½¿ç”¨
    model_path = os.path.join(RES_DIR, f"{FILE_PREFIX}_best_model.pkl")
    joblib.dump(best_model, model_path)
    print(f"   ğŸ’¾ [æ–°å¢] æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜: {model_path}")

    # ==========================================
    # 6. ç”Ÿæˆé«˜çº§å¯è§†åŒ–å›¾è¡¨ (5å¼ å›¾)
    # ==========================================
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆé«˜çº§å¯è§†åŒ–å›¾è¡¨...")
    
    def get_img_path(name):
        return os.path.join(IMG_DIR, f"{FILE_PREFIX}_{name}")

    # --- å›¾ 1: è¶…å‚æ•°æ€§èƒ½çƒ­åŠ›å›¾ (Heatmap) ---
    try:
        results_df = pd.DataFrame(grid_search.cv_results_)
        # èšåˆæ•°æ®: è¿™é‡Œå±•ç¤º n_estimators vs max_depth å¯¹åˆ†æ•°çš„å½±å“
        pivot_table = results_df.pivot_table(index='param_max_depth', 
                                             columns='param_n_estimators', 
                                             values='mean_test_score')
        plt.figure(figsize=(8, 6))
        sns.heatmap(pivot_table, annot=True, fmt=".4f", cmap="viridis")
        plt.title('Hyperparameter Performance (Accuracy)', fontsize=14)
        plt.xlabel('Number of Trees (n_estimators)')
        plt.ylabel('Max Depth')
        plt.tight_layout()
        plt.savefig(get_img_path("1_hyperparameter_heatmap.png"), dpi=300)
        plt.close()
    except Exception as e:
        print(f"   âš ï¸ æ— æ³•ç”Ÿæˆçƒ­åŠ›å›¾ (å¯èƒ½æ˜¯å‚æ•°ç»´åº¦ä¸è¶³): {e}")

    # --- å›¾ 2: ç‰¹å¾é‡è¦æ€§ (Feature Importance) ---
    importances = best_model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10, 6))
    sns.barplot(x=importances[indices], y=[features[i] for i in indices], palette='viridis')
    plt.title(f'Feature Importance (Best Model)', fontsize=14)
    plt.xlabel('Importance Score')
    plt.tight_layout()
    plt.savefig(get_img_path("2_feature_importance.png"), dpi=300)
    plt.close()

    # --- å›¾ 3: æ··æ·†çŸ©é˜µ (Confusion Matrix) ---
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=label_names, yticklabels=label_names)
    plt.title('Confusion Matrix (Optimized RF)', fontsize=14)
    plt.ylabel('True Label (K-Means)')
    plt.xlabel('Predicted Label (RF)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(get_img_path("3_confusion_matrix.png"), dpi=300)
    plt.close()

    # --- å›¾ 4: åˆ†ç±»æŠ¥å‘Šå¯è§†åŒ– (Report Heatmap) ---
    # å»æ‰æœ€åå‡ è¡Œ aggregate (accuracy, macro avg ç­‰)ï¼Œåªçœ‹å…·ä½“ç±»åˆ«çš„å¾—åˆ†
    heatmap_df = report_df.iloc[:-3, :3] 
    plt.figure(figsize=(10, len(label_names)*0.5 + 3))
    sns.heatmap(heatmap_df, annot=True, cmap="RdYlGn", fmt=".2f", vmin=0.8, vmax=1.0)
    plt.title('Class-wise Performance Metrics', fontsize=14)
    plt.tight_layout()
    plt.savefig(get_img_path("4_classification_report.png"), dpi=300)
    plt.close()

    # --- å›¾ 5: å­¦ä¹ æ›²çº¿ (Learning Curve) ---
    # æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ
    print("   ç»˜åˆ¶å­¦ä¹ æ›²çº¿ä¸­ (å¯èƒ½éœ€è¦å‡ ç§’)...")
    train_sizes, train_scores, test_scores = learning_curve(
        best_model, X, y, cv=3, n_jobs=-1, 
        train_sizes=np.linspace(0.1, 1.0, 5), scoring='accuracy'
    )
    
    train_mean = np.mean(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', color="r", label="Training Score")
    plt.plot(train_sizes, test_mean, 'o-', color="g", label="Cross-validation Score")
    plt.title('Learning Curve (Detect Overfitting)', fontsize=14)
    plt.xlabel('Training Examples')
    plt.ylabel('Accuracy Score')
    plt.legend(loc="best")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(get_img_path("5_learning_curve.png"), dpi=300)
    plt.close()

    print(f"   ğŸ–¼ï¸ æ‰€æœ‰ 5 å¼ å›¾è¡¨å·²ä¿å­˜è‡³: {IMG_DIR}")
    print("\nâœ… ä»»åŠ¡äºŒ [æ–¹æ¡ˆ B: éšæœºæ£®æ—å¢å¼ºç‰ˆ] è¿è¡Œå®Œæ¯•ï¼")

if __name__ == "__main__":
    run_model_B_rf_optimized()