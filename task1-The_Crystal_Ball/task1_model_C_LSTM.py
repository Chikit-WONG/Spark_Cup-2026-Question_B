import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os
import random

# ================= é…ç½®åŒºåŸŸ =================
INPUT_FILE = "./mcm26Train-B-Data_clean/task1_traffic_flow_5min.csv"
BASE_OUT_DIR = "./task1-The_Crystal_Ball/output/model_C_LSTM"
IMG_DIR = os.path.join(BASE_OUT_DIR, "images")
RES_DIR = os.path.join(BASE_OUT_DIR, "results")
FILE_PREFIX = "model_C_LSTM"

# è¶…å‚æ•° (Hyperparameters)
SEQ_LENGTH = 60      # è¾“å…¥åºåˆ—é•¿åº¦ (çœ‹è¿‡å» 60ä¸ªç‚¹/5å°æ—¶ æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªç‚¹)
HIDDEN_SIZE = 64     # éšè—å±‚ç¥ç»å…ƒæ•°é‡
NUM_LAYERS = 2       # LSTM å±‚æ•°
LEARNING_RATE = 0.001
EPOCHS = 100         # è®­ç»ƒè½®æ•°
BATCH_SIZE = 64

# ç»˜å›¾é£æ ¼
plt.style.use('bmh') 

# å›ºå®šéšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)

set_seed(42)
# ===========================================

# å®šä¹‰ LSTM æ¨¡å‹ç»“æ„
class TrafficLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=2, output_size=1):
        super(TrafficLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        # batch_first=True: è¾“å…¥æ ¼å¼ä¸º (batch, seq, feature)
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # åˆå§‹åŒ–éšè—çŠ¶æ€ (h0, c0)
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # å‰å‘ä¼ æ’­
        out, _ = self.lstm(x, (h0, c0))
        # å–åºåˆ—æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = self.fc(out[:, -1, :])
        return out

def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]
        y = data[i + seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

def run_lstm_model():
    print(f"ğŸš€ å¯åŠ¨æ–¹æ¡ˆ C (LSTM æ·±åº¦å­¦ä¹ ) é¢„æµ‹æ¨¡å‹...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   âš™ï¸ è¿è¡Œè®¾å¤‡: {device}")
    
    # 1. åˆ›å»ºç›®å½•
    for directory in [IMG_DIR, RES_DIR]:
        if not os.path.exists(directory):
            os.makedirs(directory)
    
    # 2. åŠ è½½æ•°æ®
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {INPUT_FILE}")
        return

    df = pd.read_csv(INPUT_FILE)
    df['ds'] = pd.to_datetime(df['ds'])
    
    # æå–ç›®æ ‡å€¼å¹¶è¿›è¡Œå½’ä¸€åŒ– (ç¥ç»ç½‘ç»œå¯¹æ•°å€¼èŒƒå›´éå¸¸æ•æ„Ÿï¼Œå¿…é¡»ç¼©æ”¾åˆ° 0-1)
    data = df['y'].values.reshape(-1, 1)
    scaler = MinMaxScaler(feature_range=(0, 1))
    data_scaled = scaler.fit_transform(data)
    
    # 3. åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›† (æœ€å 5 å¤©)
    test_days = 5
    # è®¡ç®—æµ‹è¯•é›†çš„è¡Œæ•°
    test_size = 5 * 24 * 12 # 5å¤© * 24å°æ—¶ * 12ä¸ª5åˆ†é’Ÿ
    train_size = len(data_scaled) - test_size
    
    # æ³¨æ„ï¼šLSTMéœ€è¦åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œæ‰€ä»¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ‡åˆ†è¦å°å¿ƒ
    train_data = data_scaled[:train_size]
    # ä¸ºäº†é¢„æµ‹æµ‹è¯•é›†çš„ç¬¬ä¸€ä¸ªç‚¹ï¼Œæˆ‘ä»¬éœ€è¦è®­ç»ƒé›†æœ€å SEQ_LENGTH ä¸ªç‚¹ä½œä¸ºè¾“å…¥
    # æ‰€ä»¥è¿™é‡Œçš„ test_data_input åŒ…å«äº†ç”¨äºç”Ÿæˆçš„ä¸Šä¸‹æ–‡
    
    print(f"   æ•°æ®æ€»é•¿: {len(data_scaled)}, è®­ç»ƒé›†: {train_size}, æµ‹è¯•é›†: {test_size}")

    # 4. æ„å»ºåºåˆ—æ•°æ® (Sliding Window)
    print("ğŸ› ï¸ æ­£åœ¨æ„å»ºæ—¶é—´åºåˆ—åˆ‡ç‰‡ (Sequence Windowing)...")
    X_train, y_train = create_sequences(train_data, SEQ_LENGTH)
    
    # è½¬æ¢ä¸º PyTorch Tensor
    X_train = torch.from_numpy(X_train).float().to(device)
    y_train = torch.from_numpy(y_train).float().to(device)

    # 5. åˆå§‹åŒ–æ¨¡å‹
    model = TrafficLSTM(input_size=1, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS).to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # 6. è®­ç»ƒæ¨¡å‹
    print(f"â³ æ­£åœ¨è®­ç»ƒ LSTM æ¨¡å‹ (å…± {EPOCHS} è½®)...")
    model.train()
    for epoch in range(EPOCHS):
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (epoch+1) % 10 == 0:
            print(f"   Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.6f}")

    print("   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # 7. é¢„æµ‹ (é€’å½’é¢„æµ‹ Recursive Prediction)
    print("ğŸ”® æ­£åœ¨è¿›è¡Œé€’å½’é¢„æµ‹ (è¿™å¯èƒ½æ¯”ç»Ÿè®¡æ¨¡å‹æ…¢)...")
    model.eval()
    
    # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šåªç”¨è®­ç»ƒé›†çš„æ•°æ®ï¼Œä¸€æ­¥æ­¥å¾€åæ¨ç®—å‡ºæœªæ¥5å¤©
    # åˆå§‹è¾“å…¥ï¼šè®­ç»ƒé›†æœ€å SEQ_LENGTH ä¸ªçœŸå®å€¼
    curr_seq = torch.from_numpy(train_data[-SEQ_LENGTH:]).float().to(device).unsqueeze(0) # Shape: (1, seq_len, 1)
    
    predictions_scaled = []
    
    with torch.no_grad():
        for _ in range(test_size):
            # é¢„æµ‹ä¸‹ä¸€æ­¥
            next_val_scaled = model(curr_seq)
            predictions_scaled.append(next_val_scaled.item())
            
            # æ›´æ–°è¾“å…¥åºåˆ—ï¼šå»æ‰æœ€è€çš„ä¸€ä¸ªï¼ŒåŠ ä¸Šæ–°é¢„æµ‹çš„ä¸€ä¸ª
            # next_val_scaled shape is (1, 1). Need to reshape/view correctly
            next_val_seq = next_val_scaled.unsqueeze(1) # Shape: (1, 1, 1)
            curr_seq = torch.cat((curr_seq[:, 1:, :], next_val_seq), dim=1)

    # 8. åå½’ä¸€åŒ– (Inverse Scaling)
    predictions = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))
    
    # æ•´ç†ç»“æœ
    # æ„é€ å®Œæ•´çš„ DataFrame
    # è®­ç»ƒé›†éƒ¨åˆ†çš„é¢„æµ‹æˆ‘ä»¬è¿™é‡Œä¸ºäº†çœäº‹æš‚ä¸å›æµ‹ï¼ˆå› ä¸ºé€’å½’å›æµ‹å¤ªæ…¢ï¼‰ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨æµ‹è¯•é›†
    # æˆ‘ä»¬ç”¨ Nan å¡«å……è®­ç»ƒé›†éƒ¨åˆ†çš„é¢„æµ‹åˆ—ï¼Œåªæ”¾æµ‹è¯•é›†ç»“æœ
    
    full_yhat = np.full(len(df), np.nan)
    full_yhat[-test_size:] = predictions.flatten()
    
    result_df = df.copy()
    result_df['yhat'] = full_yhat
    # ä¿®æ­£è´Ÿå€¼
    result_df['yhat_clean'] = np.nan_to_num(result_df['yhat']).clip(min=0)
    # æ³¨æ„ï¼šè®­ç»ƒé›†éƒ¨åˆ† yhat_clean å˜æˆäº†0ï¼Œè¿™åœ¨ç”»å›¾æ—¶è¦å°å¿ƒï¼Œåªç”»ååŠéƒ¨åˆ†

    # 9. è¯„ä¼° (åªè¯„ä¼°æµ‹è¯•é›†)
    test_res = result_df.iloc[-test_size:].copy()
    y_true = test_res['y'].values
    y_pred = test_res['yhat_clean'].values
    
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    print("\nğŸ“Š === æ–¹æ¡ˆ C (LSTM) è¯„ä¼°ç»“æœ ===")
    print(f"   MAE  : {mae:.2f}")
    print(f"   RMSE : {rmse:.2f}")
    print(f"   R2 Score: {r2:.4f}")

    # 10. ä¿å­˜ç»“æœ
    full_res_path = os.path.join(RES_DIR, f"{FILE_PREFIX}_full_forecast.csv")
    result_df.to_csv(full_res_path, index=False)
    
    comp_df = pd.DataFrame({
        'ds': test_res['ds'],
        'Actual': y_true,
        'Predicted_Clean': y_pred,
        'Error': y_true - y_pred
    })
    comp_res_path = os.path.join(RES_DIR, f"{FILE_PREFIX}_test_comparison.csv")
    comp_df.to_csv(comp_res_path, index=False)
    print(f"   ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {RES_DIR}")

    # 11. ç”Ÿæˆå¯è§†åŒ–
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    def get_img_path(name):
        return os.path.join(IMG_DIR, f"{FILE_PREFIX}_{name}")

    # å›¾ 1: å…¨å±€é¢„è§ˆ (åªç”»æœ€å7å¤©ï¼Œå› ä¸ºå…¨é‡é¢„æµ‹æ²¡åš)
    plt.figure(figsize=(14, 6))
    plot_start_idx = len(df) - (7 * 24 * 12)
    subset = result_df.iloc[plot_start_idx:]
    plt.plot(subset['ds'], subset['y'], label='Actual', color='gray', alpha=0.5)
    plt.plot(subset['ds'], subset['yhat_clean'], label='LSTM Prediction (Recursive)', color='green', linewidth=1.5)
    plt.axvline(x=subset['ds'].iloc[-(test_size)], color='orange', linestyle='--', label='Start of Recursive Forecast')
    plt.title(f'LSTM Recursive Forecast (Last 7 Days) - RMSE: {rmse:.2f}')
    plt.legend()
    plt.savefig(get_img_path("1_forecast_overview.png"), dpi=300)
    plt.close()

    # å›¾ 2: æ”¾å¤§æµ‹è¯•é›†
    plt.figure(figsize=(14, 7))
    plt.plot(test_res['ds'], test_res['y'], label='Actual', color='gray', alpha=0.6, linewidth=1.5)
    plt.plot(test_res['ds'], test_res['yhat_clean'], label='LSTM Prediction', color='green', linewidth=2)
    plt.title('Validation Zoom-in (Model C: LSTM)', fontsize=14)
    plt.legend()
    plt.savefig(get_img_path("2_test_set_zoom_in.png"), dpi=300)
    plt.close()

    # å›¾ 3: æ‹Ÿåˆæ•£ç‚¹å›¾
    plt.figure(figsize=(8, 8))
    plt.scatter(test_res['y'], test_res['yhat_clean'], alpha=0.3, color='green')
    max_val = max(test_res['y'].max(), test_res['yhat_clean'].max())
    plt.plot([0, max_val], [0, max_val], 'r--')
    plt.title('Actual vs Predicted (LSTM)', fontsize=14)
    plt.xlabel('Actual')
    plt.ylabel('Predicted')
    plt.savefig(get_img_path("3_fit_scatter.png"), dpi=300)
    plt.close()
    
    # å›¾ 4: æ®‹å·®åˆ†å¸ƒ
    residuals = test_res['y'] - test_res['yhat_clean']
    plt.figure(figsize=(10, 6))
    plt.hist(residuals, bins=50, color='green', edgecolor='black', alpha=0.7)
    plt.axvline(0, color='r', linestyle='--')
    plt.title('Error Distribution (Model C: LSTM)', fontsize=14)
    plt.savefig(get_img_path("4_error_distribution.png"), dpi=300)
    plt.close()

    # å›¾ 5: æ®‹å·®æ—¶é—´åºåˆ—
    plt.figure(figsize=(14, 6))
    plt.plot(test_res['ds'], residuals, color='green', alpha=0.8)
    plt.axhline(0, color='r', linestyle='--')
    plt.title('Residuals over Time (Model C: LSTM)', fontsize=14)
    plt.savefig(get_img_path("5_residuals_over_time.png"), dpi=300)
    plt.close()

    print(f"   ğŸ–¼ï¸ æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜è‡³: {IMG_DIR}")
    print("\nâœ… ä»»åŠ¡ä¸€ (æ–¹æ¡ˆC - LSTM) è¿è¡Œå®Œæ¯•ï¼")

if __name__ == "__main__":
    run_lstm_model()