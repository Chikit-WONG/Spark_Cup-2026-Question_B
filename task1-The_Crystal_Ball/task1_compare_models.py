import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os

# ================= é…ç½®åŒºåŸŸ =================
# å„æ¨¡å‹ç»“æœæ–‡ä»¶çš„è·¯å¾„ (æ ¹æ®ä½ ä¹‹å‰çš„è®¾å®š)
PATH_A = "./task1-The_Crystal_Ball/output/model_A_prophet/results/model_A_prophet_test_comparison.csv"
PATH_B = "./task1-The_Crystal_Ball/output/model_B_SARIMA/results/model_B_SARIMA_test_comparison.csv"
PATH_C = "./task1-The_Crystal_Ball/output/model_C_LSTM/results/model_C_LSTM_test_comparison.csv"

# å¯¹æ¯”ç»“æœè¾“å‡ºè·¯å¾„
OUT_DIR = "./task1-The_Crystal_Ball/output/comparison"
IMG_DIR = os.path.join(OUT_DIR, "images")
RES_DIR = os.path.join(OUT_DIR, "results")

# ç»˜å›¾é£æ ¼
plt.style.use('bmh') 
# ===========================================

def compare_models():
    print(f"âš”ï¸ å¯åŠ¨æ¨¡å‹ç»ˆæå¯¹æ¯”ç¨‹åº...")
    
    # 1. åˆ›å»ºè¾“å‡ºç›®å½•
    for d in [IMG_DIR, RES_DIR]:
        if not os.path.exists(d):
            os.makedirs(d)
    
    # 2. è¯»å–æ•°æ®
    models_data = {}
    try:
        if os.path.exists(PATH_A): models_data['Prophet (A)'] = pd.read_csv(PATH_A)
        if os.path.exists(PATH_B): models_data['SARIMA (B)'] = pd.read_csv(PATH_B)
        if os.path.exists(PATH_C): models_data['LSTM (C)'] = pd.read_csv(PATH_C)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å‡ºé”™: {e}")
        return

    if not models_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œå‰é¢çš„æ¨¡å‹ä»£ç ã€‚")
        return

    print(f"   å·²åŠ è½½æ¨¡å‹: {list(models_data.keys())}")

    # 3. è®¡ç®—æŒ‡æ ‡ (Metrics Calculation)
    metrics_list = []
    
    for name, df in models_data.items():
        # ç¡®ä¿æ—¶é—´åˆ—æ ¼å¼æ­£ç¡®
        df['ds'] = pd.to_datetime(df['ds'])
        
        y_true = df['Actual']
        y_pred = df['Predicted_Clean'] # ç»Ÿä¸€ä½¿ç”¨æ¸…æ´—åçš„é¢„æµ‹åˆ—
        
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)
        
        metrics_list.append({
            'Model': name,
            'MAE': mae,
            'RMSE': rmse,
            'R2_Score': r2
        })

    # è½¬æ¢ä¸º DataFrame å¹¶æ’åº (æŒ‰ RMSE ä»å°åˆ°å¤§ï¼Œè¶Šå°è¶Šå¥½)
    metrics_df = pd.DataFrame(metrics_list).sort_values(by='RMSE')
    
    # æ‰“å°å† å†›
    best_model = metrics_df.iloc[0]['Model']
    print(f"\nğŸ† ç»¼åˆè¡¨ç°æœ€ä½³æ¨¡å‹: {best_model}")
    print("\nğŸ“Š === è¯¦ç»†æŒ‡æ ‡å¯¹æ¯” ===")
    print(metrics_df.to_string(index=False))

    # ä¿å­˜æŒ‡æ ‡è¡¨
    metrics_df.to_csv(os.path.join(RES_DIR, "final_metrics_comparison.csv"), index=False)

    # ==========================================
    # 4. å¯è§†åŒ–å¯¹æ¯” (Visualization)
    # ==========================================
    print("\nğŸ¨ æ­£åœ¨ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")

    # --- å›¾ 1: é¢„æµ‹æ›²çº¿å¯¹æ¯” (Zoom-in) ---
    plt.figure(figsize=(15, 8))
    
    # ç”»çœŸå®å€¼ (åªç”»ä¸€æ¬¡ï¼Œå–ç¬¬ä¸€ä¸ªæ¨¡å‹çš„æ—¶é—´è½´)
    first_model_df = list(models_data.values())[0]
    plt.plot(first_model_df['ds'], first_model_df['Actual'], label='Ground Truth', color='black', linewidth=2, alpha=0.3)
    
    # ç”»å„æ¨¡å‹é¢„æµ‹å€¼
    colors = {'Prophet (A)': '#d62728', 'SARIMA (B)': '#1f77b4', 'LSTM (C)': '#2ca02c'}
    linestyles = {'Prophet (A)': '--', 'SARIMA (B)': '-', 'LSTM (C)': ':'}
    
    for name, df in models_data.items():
        plt.plot(df['ds'], df['Predicted_Clean'], 
                 label=f'{name}', 
                 color=colors.get(name, 'blue'), 
                 linestyle=linestyles.get(name, '-'),
                 linewidth=1.5, alpha=0.8)

    plt.title('Final Showdown: Prediction Comparison (Test Set)', fontsize=16)
    plt.xlabel('Date')
    plt.ylabel('Passenger Flow (kg)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(os.path.join(IMG_DIR, "1_prediction_comparison.png"), dpi=300)
    plt.close()

    # --- å›¾ 2: RMSE æŸ±çŠ¶å›¾å¯¹æ¯” ---
    plt.figure(figsize=(10, 6))
    bars = plt.bar(metrics_df['Model'], metrics_df['RMSE'], color=['gold', 'silver', '#cd7f32'])
    
    plt.title('Model Performance Ranking (RMSE - Lower is Better)', fontsize=14)
    plt.ylabel('RMSE (kg)')
    plt.grid(axis='y', alpha=0.3)
    
    # åœ¨æŸ±å­ä¸Šæ ‡æ•°å€¼
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                 f'{height:.0f}',
                 ha='center', va='bottom')
                 
    plt.savefig(os.path.join(IMG_DIR, "2_rmse_ranking.png"), dpi=300)
    plt.close()

    print(f"   âœ… å¯¹æ¯”å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUT_DIR}")

if __name__ == "__main__":
    compare_models()